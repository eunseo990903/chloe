{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a2d0087-77b3-412e-af6d-9a824dc54228",
   "metadata": {},
   "source": [
    "### 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96ba3243-07ee-4f45-8a80-2a5938e24701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Turn your wounds into wisdom.',\n",
       " 'Life is either a daring adventure or nothing at all.',\n",
       " 'The two most important days in your life are the day you are born and the day you find out why.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "text_sample = 'Turn your wounds into wisdom.\\\n",
    "               Life is either a daring adventure or nothing at all.\\\n",
    "               The two most important days in your life are the day you are born and the day you find out why.' \n",
    "sent_tokenize(text_sample)\n",
    "\n",
    "#반환된 리스트 3개의 문장으로 된 문자열 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f78bea2-d4e6-4cce-b2a9-8dbc9bab25b9",
   "metadata": {},
   "source": [
    "### 단어 토큰화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f0b49e2-099c-4207-8d64-1beb19839d66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Turn', 'your', 'wounds', 'into', 'wisdom', '.'],\n",
       " ['Life',\n",
       "  'is',\n",
       "  'either',\n",
       "  'a',\n",
       "  'daring',\n",
       "  'adventure',\n",
       "  'or',\n",
       "  'nothing',\n",
       "  'at',\n",
       "  'all',\n",
       "  '.'],\n",
       " ['The',\n",
       "  'two',\n",
       "  'most',\n",
       "  'important',\n",
       "  'days',\n",
       "  'in',\n",
       "  'your',\n",
       "  'life',\n",
       "  'are',\n",
       "  'the',\n",
       "  'day',\n",
       "  'you',\n",
       "  'are',\n",
       "  'born',\n",
       "  'and',\n",
       "  'the',\n",
       "  'day',\n",
       "  'you',\n",
       "  'find',\n",
       "  'out',\n",
       "  'why',\n",
       "  '.']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for문\n",
    "from nltk import word_tokenize\n",
    "words=[]\n",
    "sent = sent_tokenize(text_sample)\n",
    "for i in sent:\n",
    "    words.append(word_tokenize(i)) \n",
    "\n",
    "\n",
    "#list comprehension\n",
    "[word_tokenize(i) for i in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d16995-f6cb-49b0-a015-b35de0b646d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Lemmatization 표제어 추출 \n",
    "* 원형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5567e9dc-ce86-4e9d-b9f0-93d247456dc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swim\n",
      "swim\n",
      "swim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "li = ['swimming','swam','swum']\n",
    "lemma = WordNetLemmatizer()\n",
    "for i in li:\n",
    "    print(lemma.lemmatize(i,\"v\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687415ea-a6bc-46b8-b71d-791687d8d646",
   "metadata": {},
   "source": [
    "### 희소행렬\n",
    "* COO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fccb3fb-d5b1-4aa0-b2ed-73c1e04b6011",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._coo.coo_matrix'>\n",
      "  (0, 0)\t3\n",
      "  (0, 2)\t1\n",
      "  (1, 1)\t2\n",
      "<class 'numpy.ndarray'> \n",
      " [[3 0 1]\n",
      " [0 2 0]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "import numpy as np\n",
    "# 0 이 아닌 데이터 추출\n",
    "data = np.array([3,1,2])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 array로 생성 \n",
    "row_pos = np.array([0,0,1])\n",
    "col_pos = np.array([0,2,1])\n",
    "\n",
    "# sparse 패키지의 coo_matrix를 이용하여 COO 형식으로 희소 행렬 생성\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos,col_pos)))\n",
    "\n",
    "print(type(sparse_coo))\n",
    "print(sparse_coo)\n",
    "dense01=sparse_coo.toarray()\n",
    "print(type(dense01),\"\\n\", dense01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5018a9a0-c0c9-4511-a05e-01dbc7658f1b",
   "metadata": {},
   "source": [
    "* CSR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98533e3e-e405-4949-9576-a0361c0429c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n",
      "CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "dense2 = np.array([[0,0,1,0,0,5],\n",
    "             [1,4,0,3,2,5],\n",
    "             [0,6,0,3,0,0],\n",
    "             [2,0,0,0,0,0],\n",
    "             [0,0,0,7,0,8],\n",
    "             [1,0,0,0,0,0]])\n",
    "\n",
    "# 0 이 아닌 데이터 추출\n",
    "data2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 array로 생성 \n",
    "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
    "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
    "\n",
    "# COO 형식으로 변환 \n",
    "sparse_coo = sparse.coo_matrix((data2, (row_pos,col_pos)))\n",
    "\n",
    "# 행 위치 배열의 고유한 값들의 시작 위치 인덱스를 배열로 생성\n",
    "row_pos_ind = np.array([0, 2, 7, 9, 10, 12, 13])\n",
    "\n",
    "# CSR 형식으로 변환 \n",
    "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
    "\n",
    "print('COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_coo.toarray())\n",
    "print('CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8474f0d3-d4e6-41bc-806b-ac8dd01ef08f",
   "metadata": {},
   "source": [
    "### Word2Vec 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57b4a292-5340-4e30-9548-ddb012b0ce97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['plot',\n",
       " ':',\n",
       " 'two',\n",
       " 'teen',\n",
       " 'couples',\n",
       " 'go',\n",
       " 'to',\n",
       " 'a',\n",
       " 'church',\n",
       " 'party',\n",
       " ',',\n",
       " 'drink',\n",
       " 'and',\n",
       " 'then',\n",
       " 'drive',\n",
       " '.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "sentences = [list(s) for s in movie_reviews.sents()]\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7864e7-226c-4924-a0d4-edb8daf2e378",
   "metadata": {},
   "source": [
    "## 감성분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2d9f1d96-7559-47e6-bb17-1846f756eb8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_sample = 'Turn your wounds into wisdom.\\\n",
    "               Life is either a daring adventure or nothing at all.\\\n",
    "               The two most important days in your life are the day you are born and the day you find out why.' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e9210e9-0016-428b-be55-0061c946ca34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.751, 'pos': 0.249, 'compound': 0.8516}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "senti_analyzer=SentimentIntensityAnalyzer()\n",
    "senti_scores=senti_analyzer.polarity_scores(text_ sample)\n",
    "senti_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bba5e44-b5e4-4ec3-ad74-70579592aa2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 코사인 유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "184f550c-70e6-463a-8eeb-9de34ce5c48f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array((1,2)).dot(np.array((2,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "719f537e-7a84-4752-abb6-4a121610a409",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9838699100999074"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(3,4) , (1,2)의 코사인 유사도\n",
    "(np.array((1,2))@np.array((3,4))) / (np.sqrt(5) * np.sqrt(25)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3b639590-7bd7-434d-8901-f1193d08461e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7999999999999998"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = np.array((1,2))\n",
    "p2 = np.array((2,1))\n",
    "\n",
    "def cosine_similarity(p1, p2):\n",
    "        return (p1 @ p2) / (np.sqrt(sum(np.square(p1)))*np.sqrt(sum(np.square(p2))))\n",
    "cosine_similarity(p1, p2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a382d3-02cd-463c-8fc4-40d0ab73f805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
