{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb1a0969-d5ea-4723-9438-6edbb87b6525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ac30e01-0de4-4e54-96a4-0cc0dc8ab54e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>소제목</th>\n",
       "      <th>첫줄</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>실종자 600명 넘어 추가 피해 우려…부검 결과 구타·질식·장기 적출 사례도</td>\n",
       "      <td>케냐에서 '예수를 만나려면 굶어 죽으라'는 주장을 펼친 것으로 알려진 사이비 종교 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>전쟁 뒤 처음으로 교황 만나 러시아 규탄 요청…교황청 \"평화 위해 기도\"</td>\n",
       "      <td>프란치스코 교황과 볼로디미르 젤렌스키 우크라이나 대통령이 13일(현지시각) 러시아의...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[김재명의 전쟁범죄 이야기 19] 누구를 위한 '역사전쟁'인가 (下④)</td>\n",
       "      <td>\"나쁜 일을 하지 않았다. 그러니 죄의식을 가질 필요 없다.\" 이즈음 일본의 극우파...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          소제목  \\\n",
       "0  실종자 600명 넘어 추가 피해 우려…부검 결과 구타·질식·장기 적출 사례도   \n",
       "1    전쟁 뒤 처음으로 교황 만나 러시아 규탄 요청…교황청 \"평화 위해 기도\"   \n",
       "2   [김재명의 전쟁범죄 이야기 19] 누구를 위한 '역사전쟁'인가 (下④)     \n",
       "\n",
       "                                                  첫줄  \n",
       "0  케냐에서 '예수를 만나려면 굶어 죽으라'는 주장을 펼친 것으로 알려진 사이비 종교 ...  \n",
       "1  프란치스코 교황과 볼로디미르 젤렌스키 우크라이나 대통령이 13일(현지시각) 러시아의...  \n",
       "2  \"나쁜 일을 하지 않았다. 그러니 죄의식을 가질 필요 없다.\" 이즈음 일본의 극우파...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "li = []\n",
    "li1 = []\n",
    "for i in range(1,6):\n",
    "    url = 'https://www.pressian.com/pages/news-world-list?page=' + str(i)\n",
    "    html = urllib.request.urlopen(url).read().decode(encoding = 'utf-8')  #html가져오기(한글 불러오기 위해 decode 설정)\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    titles = soup.find_all(class_ = \"body\" )\n",
    "    for title  in titles:\n",
    "        li.append(title.text)\n",
    "        \n",
    "    sub_titles = soup.find_all(class_ = \"sub_title\" )\n",
    "    for sub_title  in sub_titles:\n",
    "        li1.append(sub_title.text)    \n",
    "\n",
    "df = pd.DataFrame({'소제목': li1, '첫줄':li})\n",
    "df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70aec937-2475-4f52-bd74-64d7c64abc1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>소제목</th>\n",
       "      <th>첫줄</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fifty Fifty's 'Cupid' climbs to No. 8 on the B...</td>\n",
       "      <td>Fifty Fifty's 'Cupid' climbs to No. 8 on the B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fifty Fifty's 'Cupid' lands at No. 19 on Billb...</td>\n",
       "      <td>Fifty Fifty's 'Cupid' lands at No. 19 on Billb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>피프티 피프티, 'CUPID'로 쓴 新기록..英 오피셜차트 8위</td>\n",
       "      <td>피프티 피프티, 'CUPID'로 쓴 新기록..英 오피셜차트 8위</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[INTERVIEW] FIFTY FIFTY's chief producer SIAHN...</td>\n",
       "      <td>[INTERVIEW] FIFTY FIFTY's chief producer SIAHN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>브루노 마스가 '하입 보이'와 '큐피드'를?</td>\n",
       "      <td>브루노 마스가 '하입 보이'와 '큐피드'를?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 소제목  \\\n",
       "0  Fifty Fifty's 'Cupid' climbs to No. 8 on the B...   \n",
       "1  Fifty Fifty's 'Cupid' lands at No. 19 on Billb...   \n",
       "2                피프티 피프티, 'CUPID'로 쓴 新기록..英 오피셜차트 8위   \n",
       "3  [INTERVIEW] FIFTY FIFTY's chief producer SIAHN...   \n",
       "4                           브루노 마스가 '하입 보이'와 '큐피드'를?   \n",
       "\n",
       "                                                  첫줄  \n",
       "0  Fifty Fifty's 'Cupid' climbs to No. 8 on the B...  \n",
       "1  Fifty Fifty's 'Cupid' lands at No. 19 on Billb...  \n",
       "2                피프티 피프티, 'CUPID'로 쓴 新기록..英 오피셜차트 8위  \n",
       "3  [INTERVIEW] FIFTY FIFTY's chief producer SIAHN...  \n",
       "4                           브루노 마스가 '하입 보이'와 '큐피드'를?  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = []\n",
    "li1 = []\n",
    "\n",
    "for i in range(1,6):\n",
    "\n",
    "    url = 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query=fifty%20fifty&sort=0&photo=0&field=0&pd=0&ds=&de=&cluster_rank=43&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:all,a:all&start=' + str(i)\n",
    "    html = urllib.request.urlopen(url).read().decode(encoding='utf-8')\n",
    "  \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    titles = soup.find_all(class_=\"news_tit\")\n",
    "    for title in titles:\n",
    "        li.append(title.text)\n",
    "    \n",
    "    sub_titles = soup.find_all(class_=\"api_txt_lines dsc_txt_wrap\")\n",
    "    for sub_title in titles:\n",
    "        li1.append(sub_title.text)\n",
    "    \n",
    "                       \n",
    "df = pd.DataFrame({'소제목': li1, '첫줄': li})\n",
    "\n",
    "df[:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f06e9deb-a8f4-452a-827d-8aaaf78a37ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "검색어 입력: 블핑\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>소제목</th>\n",
       "      <th>첫줄</th>\n",
       "      <th>링크</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>신세계인터내셔날(031430)은 프랑스 럭셔리 패션하우스 꾸레쥬와 국내 유통 계약을...</td>\n",
       "      <td>'블핑 제니 픽' 신세계인터, 꾸레쥬 국내 첫 매장 연다</td>\n",
       "      <td>http://www.edaily.co.kr/news/newspath.asp?news...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>매주 월요일 아침, 빠르게 변하는 주식 시장에서 주목할 종목을 짚어 드립니다. 한 ...</td>\n",
       "      <td>올해 62% 뛴 YG엔터…블핑 동생 ‘베몬’에 쏠린 눈 [이코노 株인공]</td>\n",
       "      <td>https://economist.co.kr/article/view/ecn202305...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>현대차증권은 와이지엔터테인먼트(122870)에 대해 “블랙핑크가 나홀로 어닝서프라이...</td>\n",
       "      <td>와이지엔터, 잘나가는 블핑에 짐 덜어줄 베몬까지…목표가↑-현대차</td>\n",
       "      <td>http://www.edaily.co.kr/news/newspath.asp?news...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 소제목  \\\n",
       "0  신세계인터내셔날(031430)은 프랑스 럭셔리 패션하우스 꾸레쥬와 국내 유통 계약을...   \n",
       "1  매주 월요일 아침, 빠르게 변하는 주식 시장에서 주목할 종목을 짚어 드립니다. 한 ...   \n",
       "2  현대차증권은 와이지엔터테인먼트(122870)에 대해 “블랙핑크가 나홀로 어닝서프라이...   \n",
       "\n",
       "                                         첫줄  \\\n",
       "0           '블핑 제니 픽' 신세계인터, 꾸레쥬 국내 첫 매장 연다   \n",
       "1  올해 62% 뛴 YG엔터…블핑 동생 ‘베몬’에 쏠린 눈 [이코노 株인공]   \n",
       "2       와이지엔터, 잘나가는 블핑에 짐 덜어줄 베몬까지…목표가↑-현대차   \n",
       "\n",
       "                                                  링크  \n",
       "0  http://www.edaily.co.kr/news/newspath.asp?news...  \n",
       "1  https://economist.co.kr/article/view/ecn202305...  \n",
       "2  http://www.edaily.co.kr/news/newspath.asp?news...  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#검색어 입력  -> 네이버 기사 찾기-> 링크포함 \n",
    "import urllib.request\n",
    "from bs4  import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import quote #search 에 한글 검색 오류 안나게 \n",
    "li = [] \n",
    "li1 = []\n",
    "li2=[]\n",
    "url1 = 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query='\n",
    "search = quote(str(input('검색어 입력:')))\n",
    "url2 = '&sort=0&photo=0&field=0&pd=0&ds=&de=&cluster_rank=43&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:all,a:all&start='\n",
    "\n",
    "for i in range(1,6):\n",
    "    url = url1 + search + url2 + str(i)   \n",
    "    html = urllib.request.urlopen(url).read().decode(encoding='utf-8')\n",
    "    \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    titles = soup.find_all(class_ = \"news_tit\" )\n",
    "    for title  in titles:\n",
    "        li.append(title.text)\n",
    "        \n",
    "    sub_titles = soup.find_all(class_ = \"api_txt_lines dsc_txt_wrap\" )\n",
    "    for sub_title  in sub_titles:\n",
    "        li1.append(sub_title.text)  \n",
    "        \n",
    "    for i in range(len(title        li2.append(titles[i].attrs['href'])\n",
    "\n",
    "df = pd.DataFrame({'소제목': li1, '첫줄':li,'링크': li2})\n",
    "\n",
    "df [:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e7e293a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "검색어 입력: 블핑\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>소제목</th>\n",
       "      <th>첫줄</th>\n",
       "      <th>링크</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>신세계인터내셔날(031430)은 프랑스 럭셔리 패션하우스 꾸레쥬와 국내 유통 계약을...</td>\n",
       "      <td>'블핑 제니 픽' 신세계인터, 꾸레쥬 국내 첫 매장 연다</td>\n",
       "      <td>http://www.edaily.co.kr/news/newspath.asp?news...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>매주 월요일 아침, 빠르게 변하는 주식 시장에서 주목할 종목을 짚어 드립니다. 한 ...</td>\n",
       "      <td>올해 62% 뛴 YG엔터…블핑 동생 ‘베몬’에 쏠린 눈 [이코노 株인공]</td>\n",
       "      <td>https://economist.co.kr/article/view/ecn202305...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>현대차증권은 와이지엔터테인먼트(122870)에 대해 “블랙핑크가 나홀로 어닝서프라이...</td>\n",
       "      <td>와이지엔터, 잘나가는 블핑에 짐 덜어줄 베몬까지…목표가↑-현대차</td>\n",
       "      <td>http://www.edaily.co.kr/news/newspath.asp?news...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 소제목  \\\n",
       "0  신세계인터내셔날(031430)은 프랑스 럭셔리 패션하우스 꾸레쥬와 국내 유통 계약을...   \n",
       "1  매주 월요일 아침, 빠르게 변하는 주식 시장에서 주목할 종목을 짚어 드립니다. 한 ...   \n",
       "2  현대차증권은 와이지엔터테인먼트(122870)에 대해 “블랙핑크가 나홀로 어닝서프라이...   \n",
       "\n",
       "                                         첫줄  \\\n",
       "0           '블핑 제니 픽' 신세계인터, 꾸레쥬 국내 첫 매장 연다   \n",
       "1  올해 62% 뛴 YG엔터…블핑 동생 ‘베몬’에 쏠린 눈 [이코노 株인공]   \n",
       "2       와이지엔터, 잘나가는 블핑에 짐 덜어줄 베몬까지…목표가↑-현대차   \n",
       "\n",
       "                                                  링크  \n",
       "0  http://www.edaily.co.kr/news/newspath.asp?news...  \n",
       "1  https://economist.co.kr/article/view/ecn202305...  \n",
       "2  http://www.edaily.co.kr/news/newspath.asp?news...  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#검색어 입력  -> 네이버 기사 찾기-> 링크포함 \n",
    "import urllib.request\n",
    "from bs4  import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import quote #search 에 한글 검색 오류 안나게 \n",
    "li = [] \n",
    "li1 = []\n",
    "li2=[]\n",
    "url1 = 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query='\n",
    "search = quote(str(input('검색어 입력:')))\n",
    "url2 = '&sort=0&photo=0&field=0&pd=0&ds=&de=&cluster_rank=43&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:all,a:all&start='\n",
    "\n",
    "for i in range(1,6):\n",
    "    url = url1 + search + url2 + str(i)   \n",
    "    html = urllib.request.urlopen(url).read().decode(encoding='utf-8')\n",
    "    \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    titles = soup.find_all(class_ = \"news_tit\" )\n",
    "    for title  in titles:\n",
    "        li.append(title.text)\n",
    "        \n",
    "    sub_titles = soup.find_all(class_ = \"api_txt_lines dsc_txt_wrap\" )\n",
    "    for sub_title  in sub_titles:\n",
    "        li1.append(sub_title.text)  \n",
    "        \n",
    "    for i in range(len(title        li2.append(titles[i].attrs['href'])\n",
    "\n",
    "df = pd.DataFrame({'소제목': li1, '첫줄':li,'링크': li2})\n",
    "\n",
    "df [:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "05824bc2-28af-4d74-b1de-0aaa59dacc5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"브루노 마스가 '하입 보이'와 '큐피드'를?\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#attrs 속성 \n",
    "titles[0].text\n",
    "titles[0].attrs['title']\n",
    "titles[0]['title']\n",
    "titles[0].string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8bfafeba-d9fb-4079-8ca4-951972d205a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "브루노 마스가 '하입 보이'와 '큐피드'를?\n",
      "http://www.newsis.com/view/?id=NISX20230512_0002301195&cID=50101&pID=50100\n",
      "\n",
      "Fifty Fifty becomes 1st K-pop girl group to enter British Official Singl...\n",
      "https://en.yna.co.kr/view/AEN20230506000700315?input=2106m\n",
      "\n",
      "피프티 피프티, 블랙핑크 대기록 진짜 눈앞 [이슈&톡]\n",
      "http://www.tvdaily.co.kr/read.php3?aid=16836856631674508010\n",
      "\n",
      "표절 의혹 이어지는 K팝… \"따라하지 않았다\"\n",
      "http://moneys.mt.co.kr/news/mwView.php?no=2023051217260646592\n",
      "\n",
      "Girl group Fifty Fifty ranks No. 41 on Billboard Hot 100\n",
      "https://en.yna.co.kr/view/AEN20230503004000315?input=2106m\n",
      "\n",
      "[단독]‘중소의 기적’ 피프티피프티 성공 뒤에 손목시계 자동차까지 판 ‘환...\n",
      "https://www.sportsseoul.com/news/read/1312446?ref=naver\n",
      "\n",
      "Fifty Fifty lands No. 9 on Official UK Top 40 with 'Cupid'\n",
      "https://koreajoongangdaily.joins.com/2023/05/07/entertainment/kpop/Fifty-Fifty-FiftyFifty-SaenaFifty-Fifty/20230507154718987.html\n",
      "\n",
      "피프티피프티, 美 빌보드 ‘핫100’ 19위\n",
      "https://www.munhwa.com/news/view.html?no=2023051001039912069006\n",
      "\n",
      "피프티 피프티가 FIFTY 했다 ‘핫100’ 50위 껑충\n",
      "http://sports.khan.co.kr/news/sk_index.html?art_id=202304260753003&sec_id=540301&pt=nv\n",
      "\n",
      "［韓流］ＦＩＦＴＹ ＦＩＦＴＹがビルボード４１位に上昇 ６週連続ランクイン\n",
      "https://jp.yna.co.kr/view/AJP20230503001500882?input=1728m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for title  in titles:\n",
    "    print(title.text)\n",
    "    print(title.attrs['href'])\n",
    "    print() #띄어쓰기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dd34482c-c2fe-45fd-a00f-1616b4a41dc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.newsis.com/view/?id=NISX20230512_0002301195&cID=50101&pID=50100'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[0].attrs['href']\n",
    "titles[0]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "31d2343a-5ce3-4884-bdff-ac78998c59e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>소제목</th>\n",
       "      <th>첫줄</th>\n",
       "      <th>링크</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [소제목, 첫줄, 링크]\n",
       "Index: []"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = []\n",
    "li1 = []\n",
    "li2= []\n",
    "for i in range(1,11):\n",
    "    headers={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36 Edg/113.0.1774.35'}\n",
    "    url = 'https://search.daum.net/search?w=news&DA=PGD&enc=utf8&cluster=y&cluster_page=1&q=%EC%9A%B0%ED%81%AC%EB%9D%BC%EC%9D%B4%EB%82%98&p=' + str(i)\n",
    "    html = requests.get(url, headers= headers) \n",
    "  \n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "            #response 오류뜨면 .text 붙이기 \n",
    "    \n",
    "    titles = soup.find_all(class_=\"tit_main fn_tit_u\")\n",
    "    for title in titles:\n",
    "        li.append(title.text)\n",
    "    \n",
    "    sub_titles = soup.find_all(class_=\"desc\")\n",
    "    for sub_title in titles:\n",
    "        li1.append(sub_title.text)\n",
    "    \n",
    "    for i in range(len(titles)):\n",
    "        li2.append(titles[i].attrs['href'])\n",
    "df = pd.DataFrame({'소제목': li, '첫줄': li1, '링크':li2})\n",
    "df[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b1e50a-5a11-4d11-a0db-25be9790078f",
   "metadata": {},
   "source": [
    "## 검색 이미지 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "06482bd5-c19e-4e8c-8d6b-5fb02bdfaa39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "검색어:  사과\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By \n",
    "from urllib.parse import quote_plus #search 에 한글 검색 오류 안나게 \n",
    "from selenium import webdriver\n",
    "driver = webdriver.Chrome()\n",
    "url = 'https://www.naver.com'\n",
    "driver.get(url)\n",
    "base_url='https://search.naver.com/search.naver?where=image&sm=tab_jum&query='\n",
    "plus_url= quote_plus(input('검색어: '))\n",
    "url = base_url+ plus_url\n",
    "driver.get(url)\n",
    "images= driver.find_elements(By.CLASS_NAME, \"_image._listImage\") #스페이스바점 붙이기\n",
    "images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "9567fe77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12092\\2001990389.py:6: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(\"C:/Users/user/test\")\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome(\"C:/Users/user/test\") \n",
    "#사과 url\n",
    "url = \"https://search.naver.com/search.naver?where=image&sm=tab_jum&query=%EC%82%AC%EA%B3%BC&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall\"\n",
    "driver.get(url) #해당 url 열기 \n",
    "imgs = driver.find_elements(By.TAG_NAME, \"img\")\n",
    "\n",
    "for image in images:\n",
    "    image_url = image.get_attribute('src')\n",
    "#for img in imgs:\n",
    "    #image_url= image.get_attribute('src')\n",
    "      \n",
    "    keyword = 'best_'\n",
    "  #  with urlopen(image_url) as f: \n",
    "   #         with open('./images/'+ keyword + str(number) + plus_url+ \".jpg\", \"wb\") as h:  # 이미지는 binary\n",
    "               # img = f.read()\n",
    "                #h.write(img) \n",
    "\n",
    "    \n",
    "    urlretrieve(image_url,'./images/'+ keyword + str(number) + plus_url+ \".jpg\")\n",
    "    number += 1\n",
    "    \n",
    "                  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "8c1f163d-083e-4b9a-acfc-81ecdc548407",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from urllib.request  import urlretrieve\n",
    "for image in images:\n",
    "    image_url = image.get_attribute('src')\n",
    "    \n",
    "    # 저장하기 \n",
    "    urlretrieve(image_url, './images/'+ keyword + str(number) +\".jpg\" )\n",
    " \n",
    "    number += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "7202bb23-8766-42e3-b3e4-d2b8ea1a8040",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for image in images:\n",
    "    image_url = image.get_attribute('src')\n",
    "    \n",
    "    urlretrieve(image_url,'./images/'+ keyword + str(number) + plus_url+ \".jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c158c4d2-5485-4cbc-a000-730b0b1a2497",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url='https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query=%EC%82%AC%EA%B3%BC'\n",
    "images= driver.find_elements(By.CLASS_NAME, \"_image._listImage\") #스페이스바점 붙이기\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "1644666d-63ad-4020-9e20-a0e78fad46d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "검색어:  사과\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\test\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from urllib.parse import quote_plus\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# 검색어\n",
    "search_term = input('검색어: ')\n",
    "\n",
    "# URL 인코딩\n",
    "search_term = quote_plus(search_term)\n",
    "\n",
    "# URL 생성\n",
    "url = f'https://search.naver.com/search.naver?where=image&sm=tab_jum&query={search_term}'\n",
    "\n",
    "# 웹 드라이버 초기화 (Chrome 사용)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# 웹사이트 접속\n",
    "driver.get(url)\n",
    "\n",
    "try:\n",
    "    # 이미지 요소 선택 (예: 첫 번째 이미지)\n",
    "    # WebDriverWait와 expected_conditions를 이용하여 이미지가 로드될 때까지 대기\n",
    "    img = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, 'img._image._listImage'))\n",
    "    )\n",
    "\n",
    "    # 이미지 URL 가져오기\n",
    "    img_url = img.get_attribute('src')\n",
    "\n",
    "    # 이미지를 저장할 요청을 위해 requests 라이브러리 사용\n",
    "    response = requests.get(img_url)\n",
    "\n",
    "    # 디렉토리 생성\n",
    "    if not os.path.exists('images'):\n",
    "        os.makedirs('images')\n",
    "\n",
    "    # 이진 형식으로 이미지 쓰기\n",
    "    with open(f'images/{search_term}.jpg', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "finally:\n",
    "    # 드라이버 종료\n",
    "    driver.quit()\n",
    "\n",
    "    \n",
    "import os\n",
    "print(os.getcwd()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "7b799913-cb68-4312-b2ad-4d643cfca899",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "검색어를 입력하세요:  사과\n",
      "저장할 이미지 개수를 입력하세요:  2\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from urllib.request import urlretrieve \n",
    "import os \n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "# 검색어와 저장할 이미지 개수 입력받기\n",
    "search = input(\"검색어를 입력하세요: \")\n",
    "num_images = int(input(\"저장할 이미지 개수를 입력하세요: \"))\n",
    "\n",
    "# 검색어를 URL 인코딩\n",
    "search = quote_plus(search)\n",
    "\n",
    "# 웹 드라이버 실행\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# 네이버 이미지 검색 URL에 검색어 적용\n",
    "url = f\"https://search.naver.com/search.naver?where=image&sm=tab_jum&query={search}\"\n",
    "driver.get(url)\n",
    "\n",
    "# 이미지 저장 경로 설정\n",
    "if not os.path.exists('images'):\n",
    "    os.makedirs('images')\n",
    "\n",
    "# 이미지 개수만큼 반복\n",
    "for i in range(num_images):\n",
    "    try:\n",
    "        # 이미지 요소 찾기\n",
    "        img = WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'img._image._listImage')))[i]\n",
    "        # 이미지 URL 가져오기\n",
    "        img_url = img.get_attribute('src')\n",
    "        # 이미지 저장\n",
    "        urlretrieve(img_url, f'images/img_{i}.jpg')\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        break\n",
    "\n",
    "# 웹 드라이버 종료\n",
    "driver.quit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "aab8d3da-6248-47bc-b808-2b6b22848232",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2613698956.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[145], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    soup.find_(class\"=image_list\")\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "html = requests.get(url).text\n",
    "soup=BeautifulSoup(html, 'html.parser')\n",
    "soup.find_(class\"_image_listImage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159781e0-6e9a-4414-a0c4-ce1bf36207a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "- IndexError: list index out of range 에러 - soup 있으나 titles가 없음 \n",
    "- 서버에서 처리하는것과 웹브라우저에서 받은 클래스가 다름 -> 가짜 클래스\n",
    "- 동아일보 , 알라딘 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b91cc375-9ad1-446b-b0ac-1f26d9dca1e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "48fc5c69-4a0b-454b-aad1-91c8e696700a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#크롬 열기 (웹드라이버 다운받아야함)\n",
    "from selenium import webdriver\n",
    "driver = webdriver.Chrome()\n",
    "url = 'https://www.google.com'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0ffba76b-65f9-4668-b064-277369dd5805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "driver.close() #끄기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "55a32c35-9183-4112-8930-2a206411b6bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "browser = webdriver.Chrome() # 현재파일과 동일한 경로일 경우 생략 가능\n",
    "browser.get('http://naver.com') # 네이버 사이트로 이동한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "3530402c-a169-41db-8c2b-84f953040de8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['세이노의 가르침', '각각의 계절', '도둑맞은 집중력', '모든 삶은 흐른다', '사장학개론', '나의 돈 많은 고등학교 친구', '내가 가진 것을 세상이 원하게 하라', '하늘과 바람과 별과 인간', '흔한 남매 13', '심심해서 그랬어', '주린이가 가장 알고 싶은 최다질문', '달러구트 꿈 백화점', '2030 축의 전환', '기억의 전쟁 ']\n"
     ]
    }
   ],
   "source": [
    "# 알라딘 코드 \n",
    "from urllib.request  import urlopen\n",
    "from urllib.parse import quote_plus\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver  # pip install selenium\n",
    "import time\n",
    "import requests \n",
    "list_title=[]\n",
    "#파일 가져와서 리스트로 만듬\n",
    "with open('books.txt', encoding='utf-8-sig') as f: \n",
    "    lines= f.readlines()\n",
    "    for i in lines:\n",
    "        i=i.rstrip('\\n') #\\n삭제\n",
    "        list_title.append(i)\n",
    "\n",
    "print(list_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "9ba1b553-6166-44db-9b4b-61ca29d44ad4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()\n",
    "number=1\n",
    "url='https://www.aladin.co.kr' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d616954c-2bc0-4fa2-9073-f59dd8490878",
   "metadata": {},
   "outputs": [],
   "source": [
    "for title in list_title:   \n",
    "    base_url = 'https://www.aladin.co.kr/search/wsearchresult.aspx?SearchTarget=Book&SearchWord='\n",
    "    plus_url = title\n",
    "    url = base_url +quote_plus(plus_url)\n",
    "    print(url)        \n",
    "    driver.get(url)   \n",
    "    time.sleep(1) # 1초쉬고 사이트 각각 다 들어가짐 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "32a124ab-8d25-4f71-9156-f4ff32f78936",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['세이노의 가르침', '각각의 계절', '도둑맞은 집중력', '모든 삶은 흐른다', '사장학개론', '나의 돈 많은 고등학교 친구', '내가 가진 것을 세상이 원하게 하라', '하늘과 바람과 별과 인간', '흔한 남매 13', '심심해서 그랬어', '주린이가 가장 알고 싶은 최다질문', '달러구트 꿈 백화점', '2030 축의 전환', '기억의 전쟁 ']\n",
      "https://image.aladin.co.kr/product/30929/51/cover500/s302832892_1.jpg\n",
      "https://image.aladin.co.kr/product/31599/26/cover500/8954692524_1.jpg\n",
      "https://image.aladin.co.kr/product/31559/97/cover500/k682832859_1.jpg\n",
      "https://image.aladin.co.kr/product/31381/47/cover500/k292832005_1.jpg\n",
      "https://image.aladin.co.kr/product/31325/1/cover500/k832832683_3.jpg\n",
      "https://image.aladin.co.kr/product/31515/53/cover500/k842832539_1.jpg\n",
      "https://image.aladin.co.kr/product/31495/23/cover500/k572832334_1.jpg\n",
      "https://image.aladin.co.kr/product/31600/84/cover500/k612833060_1.jpg\n",
      "https://image.aladin.co.kr/product/31527/55/cover500/k812832343_1.jpg\n",
      "https://image.aladin.co.kr/product/9/20/cover500/8985494651_1.gif\n",
      "https://image.aladin.co.kr/product/26004/14/cover500/k682737326_3.jpg\n",
      "https://image.aladin.co.kr/product/24512/70/cover500/k392630952_2.jpg\n",
      "https://image.aladin.co.kr/product/30516/90/cover500/k252830490_1.jpg\n",
      "https://image.aladin.co.kr/product/31362/6/cover500/k042832290_1.jpg\n"
     ]
    }
   ],
   "source": [
    "list_title = []\n",
    "with open(\"books.txt\", encoding='utf-8-sig') as f:    # \\ufeff 가 붙는 것을 해결\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.rstrip(\"\\n\")\n",
    "        list_title.append(line)\n",
    "print(list_title)\n",
    "\n",
    "driver  = webdriver.Chrome()\n",
    "number = 1\n",
    "\n",
    "for title in list_title:   \n",
    "    \n",
    "    base_url = 'https://www.aladin.co.kr/search/wsearchresult.aspx?SearchTarget=Book&SearchWord='\n",
    "    plus_url = title\n",
    "    url = base_url +quote_plus(plus_url)\n",
    "    # print(url)    \n",
    "        \n",
    "    driver.get(url)   \n",
    "    time.sleep(1)\n",
    "        \n",
    "    html = driver.page_source  #urlopen().read() 에 해당\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    page_1 = soup.find(class_ = 'bo3')   # 첫번째 것 한개만 가져욤  # bo3 일 지도 모른다    \n",
    "    url_2 = page_1.attrs['href'] \n",
    "\n",
    "\n",
    "    driver.get(url_2)   \n",
    "    time.sleep(1)    \n",
    "    \n",
    "    html = driver.page_source  #urlopen().read() 에 해당\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    " \n",
    "    \n",
    "    page_2 = soup.find(class_ = 'c_front')\n",
    "    image_url = page_2.img['src'] \n",
    "    print(image_url)   \n",
    "    \n",
    "      \n",
    "    # 저장하기 \n",
    "    keyword = 'best_'\n",
    "    with urlopen(image_url) as f: \n",
    "            with open('./images/'+ keyword + str(number) + plus_url+ \".jpg\", \"wb\") as h:  # 이미지는 binary\n",
    "                img = f.read()\n",
    "                h.write(img)                   \n",
    "    number += 1\n",
    "\n",
    "\n",
    "    \n",
    "driver.close()\n",
    "\n",
    "### 끝  ###### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a5d213-5411-45ca-b711-98b57618ce18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef748207-5306-4e76-91ca-455552ce98e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. 검색한 상태에서 제목, 첫두줄  10페이지 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f74b020-0c9b-4234-9f81-3f587c532b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. 링크로 들어가서 제목과 본문 가져오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "6bc4b010-44ac-4778-b508-8692cc9af126",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>소제목</th>\n",
       "      <th>첫줄</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Virality was the key to success for the K-pop ...</td>\n",
       "      <td>Hit by 'Cupid,' global listeners are head over...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Girl group Fifty Fifty’s song “Cupid” has been...</td>\n",
       "      <td>Fifty Fifty’s 'Cupid' remains on U.K. Official...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fifty Fifty-music charts Fifty Fifty's 'Cupid'...</td>\n",
       "      <td>Fifty Fifty's 'Cupid' climbs to No. 8 on the B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FIFTY FIFTY has ranked at No. 19 on the Billbo...</td>\n",
       "      <td>FIFTY FIFTY ranks No. 19 on Billboard Hot 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>그룹 피프티 피프티(FIFTY FIFTY)의 'CUPID'가 영국 '오피셜 싱글차트...</td>\n",
       "      <td>피프티 피프티, 'CUPID'로 쓴 新기록..英 오피셜차트 8위</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 소제목  \\\n",
       "0  Virality was the key to success for the K-pop ...   \n",
       "1  Girl group Fifty Fifty’s song “Cupid” has been...   \n",
       "2  Fifty Fifty-music charts Fifty Fifty's 'Cupid'...   \n",
       "3  FIFTY FIFTY has ranked at No. 19 on the Billbo...   \n",
       "4  그룹 피프티 피프티(FIFTY FIFTY)의 'CUPID'가 영국 '오피셜 싱글차트...   \n",
       "\n",
       "                                                  첫줄  \n",
       "0  Hit by 'Cupid,' global listeners are head over...  \n",
       "1  Fifty Fifty’s 'Cupid' remains on U.K. Official...  \n",
       "2  Fifty Fifty's 'Cupid' climbs to No. 8 on the B...  \n",
       "3      FIFTY FIFTY ranks No. 19 on Billboard Hot 100  \n",
       "4                피프티 피프티, 'CUPID'로 쓴 新기록..英 오피셜차트 8위  "
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = []\n",
    "li1= []\n",
    "\n",
    "for i in range(1,6):\n",
    "\n",
    "    url = 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query=fifty%20fifty&sort=0&photo=0&field=0&pd=0&ds=&de=&cluster_rank=43&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:all,a:all&start=' + str(i)\n",
    "    html = urllib.request.urlopen(url).read().decode(encoding='utf-8')\n",
    "  \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    titles = soup.find_all(class_=\"news_tit\")\n",
    "    for title in titles:\n",
    "        li.append(title.text)\n",
    "    \n",
    "    subtitles = soup.find_all(class_=\"api_txt_lines dsc_txt_wrap\")\n",
    "    for subtitle in subtitles: \n",
    "        li1.append(subtitle.text)\n",
    "    \n",
    "                       \n",
    "df = pd.DataFrame({'소제목': li1, '첫줄': li})\n",
    "\n",
    "df[:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaea3935-dbf7-43e6-9c18-eec0b8c49862",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "li = []\n",
    "li1= []\n",
    "\n",
    "for i in range(1,6):\n",
    "\n",
    "    url = 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query=fifty%20fifty&sort=0&photo=0&field=0&pd=0&ds=&de=&cluster_rank=43&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:all,a:all&start=' + str(i)\n",
    "    html = urllib.request.urlopen(url).read().decode(encoding='utf-8')\n",
    "  \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    titles = soup.find_all(class_=\"news_tit\")\n",
    "    for title in titles:\n",
    "        li.append(title.text)\n",
    "    \n",
    "    subtitles = soup.find_all(class_=\"api_txt_lines dsc_txt_wrap\")\n",
    "    for subtitle in subtitles:\n",
    "        li1.append(subtitle.text)\n",
    "    \n",
    "                       \n",
    "df = pd.DataFrame({'소제목': li1, '첫줄': li})\n",
    "\n",
    "df[:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7e9095-3c16-4f9a-b08c-aaccf9d54dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_title = []\n",
    "with open(\"books.txt\", encoding='utf-8-sig') as f:    # \\ufeff 가 붙는 것을 해결\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.rstrip(\"\\n\")\n",
    "        list_title.append(line)\n",
    "print(list_title)\n",
    "\n",
    "driver  = webdriver.Chrome()\n",
    "number = 1\n",
    "\n",
    "for title in list_title:   \n",
    "\n",
    "    base_url = 'https://www.aladin.co.kr/search/wsearchresult.aspx?SearchTarget=Book&SearchWord='\n",
    "    plus_url = title\n",
    "    url = base_url +quote_plus(plus_url)\n",
    "    # print(url)    \n",
    "        \n",
    "    driver.get(url)   \n",
    "    time.sleep(1)\n",
    "        \n",
    "    html = driver.page_source  #urlopen().read() 에 해당\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    page_1 = soup.find(class_ = 'bo3')   # 첫번째 것 한개만 가져욤  # bo3 일 지도 모른다    \n",
    "    url_2 = page_1.attrs['href'] \n",
    "\n",
    "\n",
    "    driver.get(url_2)   \n",
    "    time.sleep(1)    \n",
    "    \n",
    "    html = driver.page_source  #urlopen().read() 에 해당\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    " \n",
    "    \n",
    "    page_2 = soup.find(class_ = 'c_front')\n",
    "    image_url = page_2.img['src'] \n",
    "    print(image_url)   \n",
    "    \n",
    "      \n",
    "    # 저장하기 \n",
    "    keyword = 'best_'\n",
    "    with urlopen(image_url) as f: \n",
    "            with open('./images/'+ keyword + str(number) + plus_url+ \".jpg\", \"wb\") as h:  # 이미지는 binary\n",
    "                img = f.read()\n",
    "                h.write(img)                   \n",
    "    number += 1\n",
    "\n",
    "\n",
    "    \n",
    "driver.close()\n",
    "\n",
    "### 끝  ###### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "db274fbd-9fe5-4e51-bc89-f2ef96c8ce4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "base_url = \"https://www.donga.com/news/search?p={}&query=우크라이나&check_news=93&more=1&sorting=1&search_date=1&v1=&v2=\"\n",
    "\n",
    "# 페이지 1 크롤링\n",
    "url = base_url.format(1)\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "articles = soup.find_all(\"div\", class_=\"searchList\")\n",
    "for article in articles:\n",
    "    title = article.find(\"p\", class_=\"tit\").text.strip()\n",
    "    summary = article.find(\"p\", class_=\"txt\").text.strip()\n",
    "\n",
    "    print(\"제목:\", title)\n",
    "    print(\"요약:\", summary)\n",
    "    print(\"---\")\n",
    "\n",
    "# 페이지 2 크롤링\n",
    "url = base_url.format(16)\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "articles = soup.find_all(\"div\", class_=\"searchList\")\n",
    "for article in articles:\n",
    "    title = article.find(\"p\", class_=\"tit\").text.strip()\n",
    "    summary = article.find(\"p\", class_=\"txt\").text.strip()\n",
    "\n",
    "    print(\"제목:\", title)\n",
    "    print(\"요약:\", summary)\n",
    "    print(\"---\")\n",
    "\n",
    "# 페이지 3 크롤링\n",
    "url = base_url.format(31)\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "articles = soup.find_all(\"div\", class_=\"searchList\")\n",
    "for article in articles:\n",
    "    title = article.find(\"p\", class_=\"tit\").text.strip()\n",
    "    summary = article.find(\"p\", class_=\"txt\").text.strip()\n",
    "\n",
    "    print(\"제목:\", title)\n",
    "    print(\"요약:\", summary)\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "c934d80a-cbf0-48bb-82f0-d05ff5aadb58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.28.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.11.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2022.12.7)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4) (2.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5c285-4c36-48f6-be07-69ca0a368d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
